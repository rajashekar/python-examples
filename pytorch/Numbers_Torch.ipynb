{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3062, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# Build a feed-forward network, NOTE we are not doing softmax here at end\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10))\n",
    "\n",
    "# Define the logs\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.2913, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "## Below is using NLLLoss as criterion for that we need to use LogSoftmax\n",
    "# Build a feed-forward network, \n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "# Define the logs\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get our data\n",
    "images, labels = next(iter(trainloader))\n",
    "# Flatten images\n",
    "images = images.view(images.shape[0], -1)\n",
    "\n",
    "# Forward pass, get our logits\n",
    "logits = model(images)\n",
    "# Calculate the loss with the logits and the labels\n",
    "loss = criterion(logits, labels)\n",
    "\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6659, -1.0430],\n",
      "        [ 1.8067, -0.1915]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "## example for backward \n",
    "x = torch.randn(2,2, requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.4434, 1.0879],\n",
      "        [3.2640, 0.0367]], grad_fn=<PowBackward0>)\n"
     ]
    }
   ],
   "source": [
    "y = x**2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x1247814d0>\n"
     ]
    }
   ],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2080, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y.mean()\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0968695751f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \"\"\"\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "z.backward()\n",
    "print(x.grad)\n",
    "print(x/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before backward pass: \n",
      " None\n",
      "After backward pass: \n",
      " tensor([[ 0.0011,  0.0011,  0.0011,  ...,  0.0011,  0.0011,  0.0011],\n",
      "        [-0.0012, -0.0012, -0.0012,  ..., -0.0012, -0.0012, -0.0012],\n",
      "        [ 0.0003,  0.0003,  0.0003,  ...,  0.0003,  0.0003,  0.0003],\n",
      "        ...,\n",
      "        [-0.0004, -0.0004, -0.0004,  ..., -0.0004, -0.0004, -0.0004],\n",
      "        [ 0.0017,  0.0017,  0.0017,  ...,  0.0017,  0.0017,  0.0017],\n",
      "        [-0.0004, -0.0004, -0.0004,  ..., -0.0004, -0.0004, -0.0004]])\n"
     ]
    }
   ],
   "source": [
    "print('Before backward pass: \\n', model[0].weight.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('After backward pass: \\n', model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Above is how we got gradients\n",
    "# now we need to update weights\n",
    "from torch import optim\n",
    "\n",
    "# Optimizers require the parameters to optimize and a learning rate\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial weights -  Parameter containing:\n",
      "tensor([[ 0.0276,  0.0006, -0.0041,  ...,  0.0031, -0.0267, -0.0148],\n",
      "        [ 0.0026, -0.0128,  0.0251,  ...,  0.0347,  0.0183,  0.0233],\n",
      "        [ 0.0176, -0.0180,  0.0286,  ..., -0.0016,  0.0175, -0.0052],\n",
      "        ...,\n",
      "        [-0.0343, -0.0203,  0.0039,  ..., -0.0284, -0.0012,  0.0039],\n",
      "        [-0.0116,  0.0322,  0.0217,  ..., -0.0334, -0.0246,  0.0307],\n",
      "        [ 0.0200,  0.0292, -0.0302,  ..., -0.0306,  0.0280, -0.0297]],\n",
      "       requires_grad=True)\n",
      "Gradient -  tensor([[ 0.0054,  0.0054,  0.0054,  ...,  0.0054,  0.0054,  0.0054],\n",
      "        [ 0.0059,  0.0059,  0.0059,  ...,  0.0059,  0.0059,  0.0059],\n",
      "        [-0.0013, -0.0013, -0.0013,  ..., -0.0013, -0.0013, -0.0013],\n",
      "        ...,\n",
      "        [-0.0021, -0.0021, -0.0021,  ..., -0.0021, -0.0021, -0.0021],\n",
      "        [ 0.0011,  0.0011,  0.0011,  ...,  0.0011,  0.0011,  0.0011],\n",
      "        [ 0.0012,  0.0012,  0.0012,  ...,  0.0012,  0.0012,  0.0012]])\n"
     ]
    }
   ],
   "source": [
    "print('Initial weights - ', model[0].weight)\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "images.resize_(64, 784)\n",
    "\n",
    "# Clear the gradients, do this because gradients are accumlated\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# Forward pass, then backward pass, then update weights\n",
    "output = model.forward(images)\n",
    "loss = criterion(output, labels)\n",
    "loss.backward()\n",
    "print(\"Gradient - \", model[0].weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated weights -  Parameter containing:\n",
      "tensor([[ 0.0275,  0.0006, -0.0041,  ...,  0.0030, -0.0268, -0.0149],\n",
      "        [ 0.0025, -0.0128,  0.0250,  ...,  0.0347,  0.0183,  0.0232],\n",
      "        [ 0.0176, -0.0180,  0.0286,  ..., -0.0016,  0.0176, -0.0052],\n",
      "        ...,\n",
      "        [-0.0342, -0.0203,  0.0039,  ..., -0.0284, -0.0012,  0.0039],\n",
      "        [-0.0116,  0.0322,  0.0217,  ..., -0.0334, -0.0246,  0.0307],\n",
      "        [ 0.0200,  0.0292, -0.0302,  ..., -0.0306,  0.0280, -0.0297]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# Take an update step and for new weights\n",
    "optimizer.step()\n",
    "print('Updated weights - ', model[0].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss: 2.010779548555549\n",
      "Training loss: 0.9474777879554834\n",
      "Training loss: 0.5462500247746896\n",
      "Training loss: 0.44263335619209165\n",
      "Training loss: 0.3942380581678612\n"
     ]
    }
   ],
   "source": [
    "### implementing through loop\n",
    "\n",
    "model = nn.Sequential(nn.Linear(784, 128),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(128, 64),\n",
    "                      nn.ReLU(),\n",
    "                      nn.Linear(64, 10),\n",
    "                      nn.LogSoftmax(dim=1))\n",
    "\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.003)\n",
    "\n",
    "epochs = 5\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        # Flatten MNIST images into a 784 long vector\n",
    "        images = images.view(images.shape[0],-1)\n",
    "        # Reset past gradients\n",
    "        optimizer.zero_grad()\n",
    "        # feed forward\n",
    "        output = model.forward(images)\n",
    "        # get loss\n",
    "        loss = criterion(output, labels)\n",
    "        # back propagation\n",
    "        loss.backward()\n",
    "        # update weights\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    else:\n",
    "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x648 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import helper\n",
    "\n",
    "images, labels = next(iter(trainloader))\n",
    "img = images[0].view(1, 784)\n",
    "# Turn off gradients to speed up this part\n",
    "with torch.no_grad():\n",
    "    logits = model.forward(img)\n",
    "\n",
    "# Output of the network are logits, need to take softmax for probabilities\n",
    "ps = F.softmax(logits, dim=1)\n",
    "helper.view_classify(img.view(1, 28, 28), ps)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
